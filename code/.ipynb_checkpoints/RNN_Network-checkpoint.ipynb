{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_prox_dist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 79/10000 [00:00<00:12, 783.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize E->E\n",
      "Initialize I->E\n",
      "Initialize E->I\n",
      "Initialize I->I\n",
      "Initialize Ext->I\n",
      "Normalize E->E\n",
      "Normalize I->E\n",
      "Normalize E->I\n",
      "Normalize I->I\n",
      "Initialize x_e\n",
      "Initialize x_i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 934.75it/s]\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "variables": {
     "CF_ee": "0.05",
     "CF_eext": "0.1",
     "CF_ei": "0.1",
     "CF_ie": "0.2",
     "CF_ii": "0.2",
     "N_e": "300",
     "N_ext": "9",
     "N_i": "60",
     "w_mean_pre_ext_input": "0.2",
     "w_total_ee": "0.5",
     "w_total_ei": "-1.0",
     "w_total_ie": "1.0",
     "w_total_ii": "-1.0"
    }
   },
   "source": [
    "<div style=\"width: 700px; margin: 0 auto;\">\n",
    "\\section{Learning Structure in Time with a Plastic Recurrent Neural Network}\n",
    "\\subsection{Introduction}\n",
    "<br>\n",
    "We implemented a neural network consisting of binary neurons, modelled in discrete time steps, which follows the ideas presented in \\cite{Duarte_2014}. The architecture of our network is depicted in Fig. \\ref{fig:architecture} and shall be described in further detail.\n",
    "\n",
    "<div style=\"width:500px; margin-left: auto;  margin-right: auto;  margin-top: 30px;  margin-bottom: 30px;\">\n",
    "\\begin{figure}\n",
    "\\includegraphics[width=10cm]{./plots/illustration.png}\n",
    "\\caption{\\label{fig:architecture} Architecture of the RNN}\n",
    "\\end{figure}\n",
    "</div>\n",
    "<p>\n",
    "The recurrent network consists of a population with $N_e = $ {{N_e}} exitatory units (denoted as $x_e$) and a population with $N_i = $ {{N_i}} (denoted as $x_i$) inhibitory units. Furthermore, a population of $N_{ext} = $ {{N_ext}} excitatory units ($I_j$) is interpreted as external input, where the input coming from each external unit is to be interpreted as encoding a particular feature of a sensory stream, e.g. the recognition of a particular letter or symbol.\n",
    "</p>\n",
    "<p>\n",
    "\n",
    "\\subsection{Network Details}\n",
    "<br>\n",
    "Synaptic connectivities - represented by arrows in the illustration - were initially generated from a uniform distribution and the following properties:\n",
    "</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Connection Fraction $W_{ee}$</td><td>{{CF_ee}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Connection Fraction $W_{ei}$</td><td>{{CF_ei}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Connection Fraction $W_{ie}$</td><td>{{CF_ie}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Connection Fraction $W_{ii}$</td><td>{{CF_ii}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Connection Fraction $W_{e,ext}$</td><td>{{CF_eext}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$\\langle W_{e,ext} \\rangle$</td><td>{{w_mean_pre_ext_input}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Total postsynaptic E->E input</td><td>{{w_total_ee}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Total postsynaptic I->E input</td><td>{{w_total_ei}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Total postsynaptic E->I input</td><td>{{w_total_ie}}</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Total postsynaptic I->I input</td><td>{{w_total_ii}}</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>\n",
    "The state of the neurons is updated in discrete time steps by the following equations:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{align}\n",
    "x_{e,n}(t+1) &= \\theta\\left( \\sum_{j=1}^{N_e} W_{ee,nj} x_{e,j}(t) + \\sum_{k=1}^{N_i} W_{ei,nk} x_{i,k}(t)  + \\sum_{l=1}^{N_{ext}} W_{e,ext,nl} I_{l}(t) - T_{e,n}(t) + \\xi_{e,n}(t) \\right) \\\\\n",
    "x_{i,n}(t+1) &= \\theta\\left( \\sum_{j=1}^{N_e} W_{ie,nj} x_{e,j}(t) + \\sum_{k=1}^{N_i} W_{ii,nk} x_{i,k}(t)  - T_{i,n}(t)  + \\xi_{i,n}(t)\\right)\n",
    "\\end{align}\n",
    "<br>\n",
    "where $\\theta(\\cdot)$ is the theta function and $T_e$ and $T_i$ represent additional threshold values. $\\xi_{e/i}$ are random noise terms sampled from a Gaussian distribution at each time step.\n",
    "</p>\n",
    "<p>\n",
    "To stabilize network activity, each neuron's threshold is updated each time step such that the neuron's average activity approach a given target value:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "T_{e/i,n}(t+1) = T_{e/i,n}(t) + \\mu_{IP}\\left(x_{e/i,n}(t)-r_{target,e/i,n}\\right)\n",
    "\\end{equation}\n",
    "</p>\n",
    "<br>\n",
    "where $\\mu_{IP}$ is the learning rate of this \"intrinsic plasticity\". Target rates $r_{target,e/i}$ were drawn randomly from a Gaussian distribution for each neuron and kept fiyed throughout the simulation.\n",
    "\n",
    "<!--\n",
    "w_mean_pre_ext_input = .2\n",
    "\n",
    "w_exc_min = 0.0001\n",
    "w_inh_max = -0.0001\n",
    "##\n",
    "\n",
    "## Neuron\n",
    "g_neur = 20. # gain factor of the activation function\n",
    "\n",
    "r_target_e_mu = 0.1 # mean homeostatic excitatory target firing rate\n",
    "r_target_e_sigm = 0.#2 # standard deviation of homeostatic excitatory target firing rate\n",
    "r_target_set_e = np.minimum(1.,np.maximum(0.,np.random.normal(r_target_e_mu,r_target_e_sigm,N_e)))\n",
    "\n",
    "r_target_i_mu = 0.1 # mean homeostatic inhibitory target firing rate\n",
    "r_target_i_sigm = 0.#2 # standard deviation of homeostatic inhibitory target firing rate\n",
    "r_target_set_i = np.minimum(1.,np.maximum(0.,np.random.normal(r_target_i_mu,r_target_i_sigm,N_i))) \n",
    "\n",
    "mu_IP = 0.002 # threshold adaption rate\n",
    "\n",
    "T_e_max_init = 1.\n",
    "T_i_max_init = 1.\n",
    "\n",
    "mu_mem_noise = 0.\n",
    "sigm_mem_noise = np.sqrt(0.01)\n",
    "##\n",
    "\n",
    "## Synaptic Normalization\n",
    "w_total_ee = .5#*N_e**.5 # total presynaptic E->E input\n",
    "#w_total_eext = .5 # total presynaptic Ext->E input\n",
    "w_total_ei = -1.#*N_i**.5 # total presynaptic I->E input\n",
    "w_total_ie = 1.#*N_e**.5 # total presynaptic E->I input\n",
    "w_total_ii = -1.#*N_i**.5 # total presynaptic I->I input\n",
    "##\n",
    "-->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(<a id=\"cit-Duarte_2014\" href=\"#call-Duarte_2014\">Duarte, Series <em>et al.</em>, 2014</a>) R. Duarte, P. Series and A. Morrison, ``_Self-Organized Artificial Grammar Learning in Spiking Neural Networks_'', 07 2014.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "ipynb_biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
